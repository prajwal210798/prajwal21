<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>_________CLOSURE______________REPORT_____OF_______THE_______________PROJECT______________________&#8220;PERCEPTION__________________INFORMED_____BY
_____________NAVIGATION_______FOR______________SEARCH________AND_______________RESCUE&#8221;</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="CLOUSRE_REPORT.tex"> 
<link rel="stylesheet" type="text/css" href="CLOUSRE_REPORT.css"> 
</head><body 
>
   <div class="maketitle">



<h2 class="titleHead"><span class="uline">CLOSURE REPORT OF THE PROJECT
&#8220;PERCEPTION INFORMED BY
NAVIGATION FOR SEARCH AND
RESCUE&#8221;</span></h2>
<div class="author" ></div><br />
<div class="date" ><span 
class="ecrm-1200">January 5, 2020</span></div>
   </div>
     <dl class="list1"><dt class="list">
     </dt><dd 
class="list">
     <!--l. 42--><p class="noindent" ></dd></dl>
   <h3 class="likesectionHead"><a 
 id="x1-1000"></a>1st objective:</h3>
<!--l. 46--><p class="noindent" >
     <dl class="list1"><dt class="list">
     </dt><dd 
class="list">
     <!--l. 46--><p class="noindent" ><span class="uline"><span 
class="ectt-1000">ANNOTATED</span> <span 
class="ectt-1000">VIDEO</span> <span 
class="ectt-1000">DATA</span> <span 
class="ectt-1000">SETS</span></span></dd></dl>
<!--l. 49--><p class="noindent" >Video dataset of captured data using cameras and platform:
<!--l. 51--><p class="indent" >   The video data set contains all together 60 video files comprising the
20 video files each of different Scene/Weather namely Semi urban, Rural
,Industrial at different daylight conditions. The videos are in &#8220;*.mov&#8221; file
format and each of them is approximately of a length of 5 minutes with
a 1080 P resolution with a frame rate of 24 frames per second. Name of
these videos are given in such a way that it represents its scene and daylight
conditions.

<!--l. 59--><p class="indent" >   All the videos along with YOLO annotations has been delivered at the milestone
t0+12.
<!--l. 62--><p class="indent" >   Moreover, if further annotation is done through the freely available ViTBAT []
software then one needs a converter program for converting the annotated files to
YOLO format. This can be done through the custom developed Matlab program
entitled as &#8220;Convert_ViTBAT_annotations_to_YOLO_4.m&#8221; supplied with the final
deliverables. The program takes a folder name as an input that contains the ViTBAT
annotation files and convert all possible annotations to YOLO format and outputs in
a single folder.
<!--l. 70--><p class="noindent" >
   <h3 class="likesectionHead"><a 
 id="x1-2000"></a>2ND OBJECTIVE:</h3>
<!--l. 71--><p class="noindent" >
     <dl class="list1"><dt class="list">
     </dt><dd 
class="list">
     <!--l. 71--><p class="noindent" ><span class="uline"><span 
class="ectt-1000">Survey</span> <span 
class="ectt-1000">report</span> <span 
class="ectt-1000">of</span> <span 
class="ectt-1000">literature</span> <span 
class="ectt-1000">on</span> <span 
class="ectt-1000">similar</span> <span 
class="ectt-1000">and</span> <span 
class="ectt-1000">relevant</span> <span 
class="ectt-1000">algorithms</span> <span 
class="ectt-1000">with</span> <span 
class="ectt-1000">copies</span> <span 
class="ectt-1000">of</span> <span 
class="ectt-1000">relevant</span> <span 
class="ectt-1000">references</span></span></dd></dl>
<!--l. 74--><p class="noindent" >Most of the conventional detection and tracking algorithms/systems for moving objects
includes preprocessing, background subtraction (which includes dynamic background
modeling), detection foreground object(s) (especially the moving objects),
post-processing, and tracking (with foreground boundary/mask) [1]. The tracking
process is achieved by using a combination of spatial and temporal information about
the moving object. The underlying algorithm decides about the movement of an
object based on the spatial and temporal coherency. However, the process of
background modeling is quite challenging due to the factors like dynamic
background, variation in illuminations, change in the scene, mis-classification of
shadow as an object, camouflage and bootstrapping problems [2,3] etc. Moreover,
background subtraction is quite useful for a video from a static camera but,
delays the operation of object tracking in a video of dynamic background.
Therefore, recent trends avoid the background-subtraction step and use machine
learning approaches for detection, classification and tracking of moving objects.
An input image is most often pre-processed for normalizing contrast and
brightness effects [4]. A very common preprocessing step is to subtract the
mean of image intensities and divide by the standard deviation. Sometimes,
gamma correction produces slightly better results. On the other hand, a color
space transformation (e.g. RGB to LAB color space) may help to get better
results while dealing with color images. Finally, as part of pre-processing, an
input image or patch of an image is also cropped and resized to a fixed size.
This is essential for the next step, i.e. feature extraction, which requires a
fixed sized image for its operation. However, one may remember that it
is hard to predict a particular pre-processing algorithm that best suits a
typical application. Fortunately, the good thing is that the choice of the

pre-processing algorithm affects minimally in the results, and slight betterment in the
result can only be obtained with a heuristic choice of the algorithm. The
preprocessing step is followed by the Feature Extraction. Actually, the input image
contains much extra information that is not necessary for classification.
Therefore, prior to image classification, the target image is simplified by
extracting the important information contained in the image and leaving out the
rest. For example, if one wants to find the shirt and coat buttons in images,
he/she will notice a significant variation in RGB pixel values. However, the
image can be simplified by running an edge detector on the image. It is still
easily possible to discern the circular shape of the buttons in these edge
images. Therefore, it can be concluded that edge detection retains the essential
information while throwing away non-essential information. However, designing
these features is crucial to the performance of the algorithm in a traditional
computer vision approaches. Actually, a much better performance can be
achieved finding much more reliable features than the simple edge detection.
In the example of shirt and coat buttons, a good feature detector will not
only capture the circular shape of the buttons but also gather information
about how buttons are different from other circular objects like car tires.
Some well-known features used in computer vision are Haar-like features
(introduced by Viola and Jones) [5], Histogram of Oriented Gradients (HOG)
[6,7], Scale-Invariant Feature Transform (SIFT) [8,9], Speeded Up Robust
Feature (SURF) [10] etc. Now, after converting an image to a corresponding
feature vector, classification algorithm takes this feature vector as input and
outputs a class label. However, these classification algorithms are required to
be trained before it can actually determine the classes of the objects. The
algorithms need to be trained by showing thousands of examples of images
from different target and known classes. Different learning algorithms learn
differently, but the general principle is that the learning algorithms treat
feature vectors as points in higher dimensional space, and try to find planes /
surfaces that partition the higher dimensional space in such a way that all
examples belonging to the same class are on one side of the plane / surface.
The classification algorithms/classifiers are majorly clustered in - Linear
classifier, Quadratic classifier, Bayesian classifier, Neural Network, Kernel
estimation and k-nearest neighbor, Decision Trees, Support Vector Machines,
Maximum Entropy Classifier etc. Among these algorithms, the mostly cultured
classifier is the Neural Network based classifier. Now, an object detection and
classification system takes a classifier for an object of interest and evaluates
it at various locations and scales in a test image. Different systems use a
sliding window approach where the classifier is run at evenly spaced locations
over the entire image. On the other hand, the popular R-CNN use region
proposal methods to, first, generate potential bounding boxes in an image
and then run a classifier on these proposed boxes [11]. After classification,
post-processing is used to refine the bounding box, eliminate duplicate detections,
and rescore the box based on other objects in the scene. However, these
complex pipelines are slow and hard to optimize because each individual
component must be trained separately. To resolve this limitation, some new

algorithms like YOLO [11] has been proposed which trains the classifier to find
multiple objects in a single frame in one go. The algorithm has proved its
potentiality in terms of speed and accuracy. Nevertheless, in spite of these long
sustained researches in this field, there still exists a significant probability of
false-detection and missed-detection when the object is detected in a single
frame. This is because; the current autonomous unmanned aerial platforms
simply implement a passive object detection running on the live camera
images [12]. On the other hand, Humans lingers its gaze in an object until it
is detected properly [13]. The gaze tries to obtain a temporal continuity
between the consecutive frames while the object is in motion. Otherwise, the
continuity is obtained by moving our gaze around the object to collect more
features about it. This work gathers inspiration from the above mentioned
behavior which occurs naturally across humans, birds and most animals. Some
strategies have also been proposed in the same direction [14], but they yet
required to be improved further for real-time implementation. Temporal
verification of object in continuous videos is required to improve the accuracy or
confidence of detection. Temporal consistency dictionary learning may also be
incorporated further in improvement of the tracking algorithm [15]. Multiple
frame may be captured by the platform by maneuvering it in a straight line
forward motion (dolly), or sideways motion (truck), or up-down motion
(pedestal); and their correlation may be utilized to enhance the confidence,
and hence to improve the accuracy in detection [16-19] as well as tracking
[20,21].
<!--l. 179--><p class="indent" >   Soft copies of the relevant literatures have been provided at the t0+12
milestone.
<!--l. 182--><p class="indent" >   <span class="uline">References:</span>
<!--l. 184--><p class="indent" >   [1] Bahadir Karasulu and Serdar Korukoglu,.: Springer, 2013, ch. 2, pp.
7-30.
<!--l. 187--><p class="indent" >   [2] B. Ahn, "Real-time video object recognition using convolutional neural
network," , 2015, pp. 1-7.
<!--l. 190--><p class="indent" >   [3] J. S. Kulchandani and K. J. Dangarwala, "Moving object detection: Review of
recent research trends," , 2015, pp. 1-5.
<!--l. 194--><p class="indent" >   [4] SATYA MALLICK. (2016, November) http://www.learnopencv.com. [Online].
http://www.learnopencv.com/image-recognition-and-object-detection-part1/
<!--l. 197--><p class="indent" >   [5] P. Viola and M. Jones, "Rapid object detection using a boosted cascade of
simple features," , vol. 1, 2001, pp. I-511-I-518 vol.1.
<!--l. 201--><p class="indent" >   [6] N. Dalal and B. Triggs, "Histograms of oriented gradients for human
detection," , vol. 1, 2005, pp. 886-893 vol. 1.
<!--l. 205--><p class="indent" >   [7] R.K. McConnell, Method of and apparatus for pattern recognition, 1986, US
Patent 4,567,610.
<!--l. 208--><p class="indent" >   [8] D. G. Lowe, "Object recognition from local scale-invariant features," , vol. 2,
1999, pp. 1150-1157 vol.2.
<!--l. 211--><p class="indent" >   [9] D.G. Lowe, Method and apparatus for identifying scale invariant features in an
image and use of same for locating an object in an image, 2004, US Patent
6,711,293.
<!--l. 215--><p class="indent" >   [10] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool,

"Speeded-Up Robust Features (SURF)," Computer Vision and Image Understanding,
vol. 110, pp. 346-359, 2008, Similarity Matching in Computer Vision and
Multimedia.
<!--l. 220--><p class="indent" >   [11] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi,
"You Only Look Once: Unified, Real-Time Object Detection," CoRR, vol.
abs/1506.02640, 2015.
<!--l. 224--><p class="indent" >   [12] Z. He et al., "Ard-<span 
class="grmn-1000">&#x03BC;</span>-Copter: A Simple Open Source Quadcopter Platform," ,
2015, pp. 158-164.
<!--l. 228--><p class="indent" >   [13] Kiwon Yun, Yifan Peng, Dimitris Samaras, Gregory Zelinsky, and Tamara
Berg, "Exploring the role of gaze behavior and object detection in scene
understanding," Frontiers in Psychology, vol. 4, p. 917, 2013.
<!--l. 233--><p class="indent" >   [14] B.V.K.V. Kumar, A. Mahalanobis, and R.D. Juday, Correlation Pattern
Recognition.: Cambridge University Press, 2010.
<!--l. 236--><p class="indent" >   [15] X. Cheng, Y. Zhang, J. Cui, and L. Zhou, "Object Tracking via Temporal
Consistency Dictionary Learning," IEEE Transactions on Systems, Man, and
Cybernetics: Systems, vol. 47, pp. 628-638, 2017.
<!--l. 241--><p class="indent" >   [16] R. Kerekes and B. V. K., "Enhanced Video-Based Target Detection using
Multi-Frame Correlation Filtering," IEEE Transactions on Aerospace and Electronic
Systems, vol. 45, pp. 289-307, 2009.
<!--l. 246--><p class="indent" >   [17] R. Feris, A. Datta, S. Pankanti, and M. T. Sun, "Boosting object detection
performance in crowded surveillance videos," 2013, pp. 427-432.
<!--l. 250--><p class="indent" >   [18] Michael Horton, Mike Cameron-Jones, and Raymond Williams, "Multiple
Classifier Object Detection with Confidence Measures," in AI 2007: Advances in
Artificial Intelligence: 20th Australian Joint Conference on Artificial Intelligence,
Gold Coast, Australia, December 2-6, 2007. Proceedings, Mehmet A. Orgun and
John Thornton, Eds.: Springer Berlin Heidelberg, 2007, pp. 559-568.
<!--l. 257--><p class="indent" >   [19] E. N. Mortensen and W. A. Barrett, "A confidence measure for boundary
detection and object selection," vol. 1, 2001, pp. I-477-I-484 vol.1.
<!--l. 261--><p class="indent" >   [20] L. Dong, I. Zoghlami, and S. C. Schwartz, "Object Tracking in Compressed
Video with Confidence Measures," 2006, pp. 753-756.
<!--l. 265--><p class="indent" >   [21] B. Zhang et al., "Adaptive Local Movement Modeling for Robust Object
Tracking," IEEE Transactions on Circuits and Systems for Video Technology, vol. 27,
pp. 1515-1526, 2017.
<!--l. 269--><p class="noindent" >
   <h3 class="likesectionHead"><a 
 id="x1-3000"></a>3rd objective :</h3>
     <ul class="itemize1">
     <li class="itemize">Report on implementation of object tracking for all detections
     </li>
     <li class="itemize">Report on the probabilistic approach for enhancing confidence

     </li>
     <li class="itemize">Plots of detection accuracies per frame and its comparative results</li></ul>
<!--l. 275--><p class="noindent" >In this work, object detection has been implemented using the well-known
You-Only-Look-Once (YOLO) algorithm. Like most other deep-neural-network based
detection algorithms, the YOLO algorithm detects objects in an image frame. This
excludes the possibility to relate the temporal information of an object with its
detection confidence. On the other hand, a conventional tracking algorithm cannot
detect an object, but is able to relate the present position of an arbitrary object with
its previous position. Therefore; fusion of YOLO with a tracking algorithm, e.g.
Kalman Filter based tracking, is made in this work to improve the detection
confidence of an object.
<!--l. 286--><p class="indent" >   Most of the existing object detection and tracking algorithms that are present
today for ready to use does not associate the temporal information of the object with
its detection congidence. In this work, the temporal imformation of the object has
been associated to improve the confidence of the object. In other words, machine
ability to identify an object has been enhanced through countinous gazing to the
object and thus building up the confidence to indentify the object particularly in low
light condition .
<!--l. 295--><p class="indent" >   The formula that is proposed here uses the confidences of an detected object and
assign it to a tracker for the object. Then the confidences of the successive
detections have been associated with the tracker confidence through the following
formula.
<!--l. 300--><p class="indent" >   Proposed Formula :
<!--l. 302--><p class="indent" >   C<sub><span 
class="cmmi-7">T</span></sub> = [C<sub><span 
class="cmmi-7">D</span></sub> + (C<sub><span 
class="cmmi-7">T</span><span 
class="cmsy-7">-</span><span 
class="cmr-7">1</span></sub> - C<sub><span 
class="cmmi-7">D</span></sub>)<span 
class="tcrm-1000">&#x00D7;</span>IoU]<sup><span 
class="cmmi-7">m</span></sup>
<!--l. 304--><p class="indent" >   where m= (l<sub><span 
class="cmmi-7">m</span></sub> x L + 1)/(h<sub><span 
class="cmmi-7">m</span></sub>x H + 1) where 0<span 
class="cmsy-10">&#x2264; </span>l<sub><span 
class="cmmi-7">m</span></sub>&#x003C;1, 0<span 
class="cmsy-10">&#x2264;</span>h<sub><span 
class="cmmi-7">m</span></sub>&#x003C;1
<!--l. 307--><p class="indent" >   Here, the C<sub><span 
class="cmmi-7">T</span></sub> =Tracker-Confidence in T<sup><span 
class="cmmi-7">th</span></sup>frame.
<!--l. 309--><p class="indent" >   C<sub><span 
class="cmmi-7">D</span></sub> =Detection-Confidence of the object in the T<sup><span 
class="cmmi-7">th</span></sup> frame.
<!--l. 311--><p class="indent" >   IoU = percentage of overlap within the areas of tracked prediction and detection.
Here, the m is actually responsible for improving or declining the tracker-confidence
in the present frame based on the tracking accuracy. The H (Hit) is the number of
time the tracker is able to track the targeted object and the L (Loss) is the number of
time the tracker misses the object since its inception. Moreover, h<sub><span 
class="cmmi-7">m</span></sub> and l<sub><span 
class="cmmi-7">m</span></sub> are two
weighting factors which are responsible to drive the improvement/declination to their
saturation.
<!--l. 320--><p class="indent" >   Experiments that are carried on the various lighting conditions have shown the
expected results.
<!--l. 323--><p class="indent" >   To illustrate the result let&#8217;s take the graph of an object&#8217;s tracker and detected
confidence
<!--l. 326--><p class="indent" >   <img 
src="0_media_prajwal-thakur_A2C2D91AC2D8F38F_project_drdo_DRDO_DRAFT_pasted1.png" alt="PIC"  
>
<!--l. 328--><p class="indent" >   The improved confidence /build up confidence of object is equivalent to the
confidence of the tracker .
<!--l. 331--><p class="indent" >   From frame 0 to 10 the confidence of tracker is falling as the number
of losses is increasing this is analogous to the situation where human first
think that a object is &#8220;X&#8221; with confidence of almost 0.5 then next moment
is he/she think that object is another object &#8220;Y&#8221; due to the unabilty to

track the object movement in low lighting condition or because of other
reason.
<!--l. 338--><p class="indent" >   From frame 10 to 25 tracker confidence starts rising as hits is increasing this is
analogus to the situation where a human starts to identify an object to be &#8220;X&#8221;
continuously but with low confidence may be due to lighting condition or due to any
other factors and after continuous gazing he is able to identify the object with almost
100% of confidence.
<!--l. 344--><p class="noindent" >
   <h5 class="likesubsubsectionHead"><a 
 id="x1-4000"></a>The above formula is incorporated through the following algorithm.</h5>
<!--l. 347--><p class="noindent" >
   <h5 class="likesubsubsectionHead"><a 
 id="x1-5000"></a>-------------------------------- Algorithm ----------------------------- </h5>
<!--l. 349--><p class="noindent" >Initialize the constants lm and hm.
<!--l. 351--><p class="indent" >   1. Open the video
<!--l. 353--><p class="indent" >   2. Continue the LOOP till the last frame of the video:
<!--l. 355--><p class="indent" >   2.1 Read a frame
<!--l. 357--><p class="indent" >   2.2 Detect the objects in each frame using YOLO
<!--l. 359--><p class="indent" >   2.3 Filter the interested objects (Human/Vehichles/Animals) and store them in
an Array
<!--l. 362--><p class="indent" >   2.4 Use Hungarian Algo and Intersection metric (IOU) of bounding boxes of the
Tracker and the Detector to associate existing trackers with the detected objects in
the current frame.
<!--l. 366--><p class="indent" >   2.5 If a detected object cannot be allocated to a tracker (which is obvious at the
initiation of the algorithm), then
<!--l. 369--><p class="indent" >   2.5.1 A new tracker is allocated to the detected object.
<!--l. 371--><p class="indent" >   2.5.2 The new tracker (here Kalman Filter Based tracker) is assigned a
Tracker-Confidence which is equal to the confidence of the detected object.
<!--l. 375--><p class="indent" >   2.5.3 Number of Hits (H) is set to be 1
<!--l. 377--><p class="indent" >   2.5.4 Set the Number of mis-Hits (L) to 0
<!--l. 379--><p class="indent" >   2.6 If the detected object can be assigned to an existing tracker (using step 2.4),
then
<!--l. 382--><p class="indent" >   2.6.1 Update the Tracker-Confidence based on the equation 1.
<!--l. 384--><p class="indent" >   2.6.2 Update the tracker position to improve accuracy
<!--l. 386--><p class="indent" >   2.6.3 Predict the expected position of the object in the next frame through
tracker bounding boxes
<!--l. 389--><p class="indent" >   2.6.4 Increase the Number of Hits (H)
<!--l. 391--><p class="indent" >   2.7 If a tracker cannot be assigned to an object, then
<!--l. 393--><p class="indent" >   2.7.1 Increase the Number of mis-Hits (L)
<!--l. 395--><p class="indent" >   2.7.2 If the Number of mis-Hits (L) is greater than a predefined value
(Max_Age), then
<!--l. 398--><p class="indent" >   2.7.2.1 Delete the tracker
<!--l. 400--><p class="indent" >   3 End of loop

<!--l. 402--><p class="indent" >   --------------------------------------------------------------------------------------------------------------------------
<!--l. 404--><p class="indent" >   Outcome of implementation of the above algorithm (through Python
Codes) results in the following graphs for different detected objects in different
scenes.
<!--l. 407--><p class="noindent" >
     <dl class="list1"><dt class="list">
     </dt><dd 
class="list">
     <!--l. 407--><p class="noindent" ><span 
class="ectt-1000">Task</span><span 
class="ectt-1000">&#x00A0;has</span><span 
class="ectt-1000">&#x00A0;been</span><span 
class="ectt-1000">&#x00A0;successfully</span><span 
class="ectt-1000">&#x00A0;completed</span><span 
class="ectt-1000">&#x00A0;by</span><span 
class="ectt-1000">&#x00A0;implementing</span><span 
class="ectt-1000">&#x00A0;the</span><span 
class="ectt-1000">&#x00A0;</span>
     <!--l. 410--><p class="noindent" ><span 
class="ectt-1000">proposed</span><span 
class="ectt-1000">&#x00A0;formula</span><span 
class="ectt-1000">&#x00A0;in</span><span 
class="ectt-1000">&#x00A0;different</span><span 
class="ectt-1000">&#x00A0;situations</span><span 
class="ectt-1000">&#x00A0;and</span><span 
class="ectt-1000">&#x00A0;getting</span><span 
class="ectt-1000">&#x00A0;the</span><span 
class="ectt-1000">&#x00A0;desired</span><span 
class="ectt-1000">&#x00A0;result.</span></dd></dl>
<!--l. 413--><p class="noindent" >
   <h4 class="likesubsectionHead"><a 
 id="x1-6000"></a>Source code of the implementation with related libraries </h4>
<!--l. 414--><p class="noindent" >
     <dl class="list1"><dt class="list">
     </dt><dd 
class="list">
     <!--l. 414--><p class="noindent" ><span 
class="ectt-1000">A</span><span 
class="ectt-1000">&#x00A0;Folder</span><span 
class="ectt-1000">&#x00A0;named</span><span 
class="ectt-1000">&#x00A0;Test_Codes_of_DRDO_project</span><span 
class="ectt-1000">&#x00A0;contains</span><span 
class="ectt-1000">&#x00A0;the</span><span 
class="ectt-1000">&#x00A0;codes</span><span 
class="ectt-1000">&#x00A0;for</span><span 
class="ectt-1000">&#x00A0;</span>
     <!--l. 417--><p class="noindent" ><span 
class="ectt-1000">final</span><span 
class="ectt-1000">&#x00A0;executed</span><span 
class="ectt-1000">&#x00A0;program</span><span 
class="ectt-1000">&#x00A0;.Folder</span><span 
class="ectt-1000">&#x00A0;contains</span><span 
class="ectt-1000">&#x00A0;following</span><span 
class="ectt-1000">&#x00A0;files</span></dd></dl>
<!--l. 419--><p class="noindent" ><span 
class="tcrm-1000">&#8226; </span>&#8220;data&#8221; folder
<!--l. 421--><p class="indent" >   <span 
class="tcrm-1000">&#8226; </span>&#8220;support&#8221; folder
<!--l. 423--><p class="indent" >   <span 
class="tcrm-1000">&#8226; </span>&#8220;darknet.py&#8221; file
<!--l. 425--><p class="indent" >   <span 
class="tcrm-1000">&#8226; </span>&#8220;main_code.py&#8221; file (main python file containing the algorithims for detection
and tracking)
<!--l. 429--><p class="indent" >   <span 
class="tcrm-1000">&#8226; </span>&#8220;KFtracker.py&#8221; file (Supporting file for main_code.py)
<!--l. 432--><p class="indent" >   <span 
class="tcrm-1000">&#8226; </span>&#8220;inputFiles&#8221; folder
<!--l. 434--><p class="indent" >   <span 
class="tcrm-1000">&#8226; </span>&#8220;outputFiles&#8221; folder
<!--l. 436--><p class="indent" >   Within the above list, the first two folders and the third file are supporting files
for the developed programs. The custom developed programs for the project are
given in the 4th and 5th executable python code files. . The last two folders supports
this custom developed codes by seeding video input files through &#8220;inputFiles&#8221; folder .
The &#8220;outputFiles&#8221; folder stores &#8220;boxed&#8221; video versions of the input files. It also
contains a folder named &#8220;Graph&#8221; which stores some output graphs originated from
the program.
<!--l. 446--><p class="indent" >   <span 
class="tcrm-1000">&#8226; </span>software prerequisites
     <div class="quote">
     <!--l. 448--><p class="noindent" ><span 
class="tcrm-1000">&#8226; </span>Python 3
     <!--l. 450--><p class="noindent" ><span 
class="tcrm-1000">&#8226; </span>Open CV

     <!--l. 452--><p class="noindent" >For GPU compatibility
     <!--l. 454--><p class="noindent" ><span 
class="tcrm-1000">&#8226; </span>CUDA and CUDNN
     <!--l. 456--><p class="noindent" ><span 
class="tcrm-1000">&#8226; </span>darknet</div>
<!--l. 459--><p class="noindent" >
   <h4 class="likesubsectionHead"><a 
 id="x1-7000"></a>SCOPE OF FUTURE WORK</h4>
     <ul class="itemize1">
     <li class="itemize">The choice of detection and tracking algorithim</li></ul>
<!--l. 463--><p class="noindent" >A better tracking algorithim can be adopted to improve accuracy of the proposed
algorithm. However, this is subjected to the trade-off of time complexity/frame rate
of the algorithm.
<!--l. 467--><p class="indent" >   A better detection algorithim can be used to detect to the object in low lightling
condition.
<!--l. 470--><p class="indent" >   An exhausive trining of the detection algorithm is required for currectly detecting
objects in an aerial image.
<!--l. 473--><p class="noindent" >
   <h4 class="likesubsectionHead"><a 
 id="x1-8000"></a></h4>
    
</body></html> 



